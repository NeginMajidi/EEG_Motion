{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from hdf5storage import loadmat\n",
    "import h5py\n",
    "from scipy import signal\n",
    "import _pickle as pkl\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_dir = 'D:\\Motion Data'\n",
    "mat_files_dir = os.path.join(Data_dir, 'mat_files')\n",
    "pkl_files_dir = os.path.join(Data_dir, 'pkl_files')\n",
    "morlet_files_dir = os.path.join(Data_dir, 'morlet_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make event files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_mat_files = os.listdir(mat_files_dir)\n",
    "# patient_mat_files.remove(patient_mat_files[0])\n",
    "# for patient in patient_mat_files:\n",
    "#     patient_name = patient[:-10]\n",
    "#     patient_events_dir = os.path.join(pkl_files_dir, patient_name)\n",
    "#     os.mkdir(patient_events_dir)\n",
    "#     mat_file = loadmat(os.path.join(mat_files_dir, patient))\n",
    "#     eeg = mat_file['eegdat']\n",
    "#     confidence = mat_file['events']['confidence']\n",
    "#     motion_coherence = mat_file['events']['motionCoherence']\n",
    "#     motion_direction = mat_file['events']['motionDirection']\n",
    "#     detection_accuracy = mat_file['events']['accuracy']\n",
    "#     events_num = eeg.shape[2]\n",
    "#     for event in range(events_num):\n",
    "#         event_file_name = os.path.join(patient_events_dir, 'event_{}'.format(str(event).zfill(4)))\n",
    "#         event_eeg = eeg[:,:,event]\n",
    "#         event_confidence = confidence[0, event]\n",
    "#         event_motion_coherence = motion_coherence[0, event]\n",
    "#         event_motion_direction = motion_direction[0, event]\n",
    "#         event_detection_accuracy = detection_accuracy[0, event]\n",
    "#         np_file = (event_eeg, event_confidence, event_motion_coherence, event_motion_direction, event_detection_accuracy)\n",
    "#         np.save(event_file_name, np_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make morlet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morlet_files_dir = os.path.join(Data_dir, 'morlet_files')\n",
    "# # os.mkdir(morlet_files_dir)\n",
    "# patients = os.listdir(pkl_files_dir)\n",
    "# patients.remove(patients[0])\n",
    "# for patient in patients:\n",
    "#     patient_morlet_dir = os.path.join(morlet_files_dir, patient)\n",
    "#     os.mkdir(patient_morlet_dir)\n",
    "#     patient_events_dir = os.path.join(pkl_files_dir, patient)\n",
    "#     for event in os.listdir(patient_events_dir):\n",
    "#         event_file_name = os.path.join(patient_events_dir, event)\n",
    "#         npy_file_name = event_file_name + '.npy'\n",
    "#         event_file = np.load(npy_file_name, allow_pickle=True)\n",
    "#         event_eeg = event_file[0]\n",
    "#         coef, freq = pywt.cwt(event_eeg, np.arange(1,81),'cmor0.4-1.0', sampling_period=2e-3)\n",
    "#         event_morlet = np.moveaxis(abs(coef), 0, 1)\n",
    "#         morlet_npy_file = (event_morlet, event_file[1], event_file[2], event_file[3], event_file[4])\n",
    "#         morlet_file_name = os.path.join(patient_morlet_dir, 'morlet_{}'.format(str(event).zfill(4)))\n",
    "#         np.save(morlet_file_name, morlet_npy_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "patients = os.listdir(pkl_files_dir)\n",
    "train_patients = np.random.choice(patients, int(len(patients) * 0.8), replace=False)\n",
    "test_patients = np.array(list(set(patients) - set(train_patients)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 100\n",
    "total_data_size = 20983\n",
    "total_train_size = 16431\n",
    "total_test_size = 4552\n",
    "epoch_size = total_train_size // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(x, min_value, max_value):\n",
    "#     return (x - min_value) / (max_value - min_value)\n",
    "\n",
    "# def normalizer_fn(x, y):\n",
    "#     return normalize(x), normalize(y)\n",
    "\n",
    "# def batch_generator(X, y, batch_size, preprocess_funcs=[]):\n",
    "    \n",
    "#     while True:\n",
    "#         batch_patients = np.random.choice(len(X), batch_size, replace=True)\n",
    "#         X_batch, y_batch = [], []\n",
    "#         for patient in batch_patients:\n",
    "#             patient_morlet_dir = os.path.join(morlet_files_dir, patient)\n",
    "#             random_event = np.random.choice(os.listdir(patient_morlet_dir))\n",
    "#             random_event_file_name = os.path.join(patient_morlet_dir, random_event)\n",
    "#             random_event_file = np.load(random_event_file_name, allow_pickle=True)\n",
    "#             X_batch.append(random_event_file[0])\n",
    "#             y_batch.append(random_event_file[4])\n",
    "#         X_batch, y_batch = np.array(X_batch), np.array(y_batch)\n",
    "#         for func in preprocess_funcs:\n",
    "#             X_batch, y_batch = func(X_batch, y_batch)\n",
    "#         yield np.array(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(x, min_value, max_value):\n",
    "#     return (x - min_value) / (max_value - min_value)\n",
    "\n",
    "# def normalizer_fn(x, y):\n",
    "#     return normalize(x), normalize(y)\n",
    "\n",
    "def batch_generator(X, batch_size):\n",
    "    \n",
    "    while True:\n",
    "        batch_patient = X\n",
    "        X_batch, y_batch = [], []\n",
    "        patient_morlet_dir = os.path.join(morlet_files_dir, batch_patient)\n",
    "        random_events = np.random.choice(os.listdir(patient_morlet_dir), batch_size, replace=True)\n",
    "        for random_event in random_events:\n",
    "            random_event_file_name = os.path.join(patient_morlet_dir, random_event)\n",
    "            random_event_file = np.load(random_event_file_name, allow_pickle=True)\n",
    "            X_batch.append(random_event_file[0][:,:,:10])\n",
    "            y_batch.append(np.squeeze(random_event_file[4]))\n",
    "#         X_batch, y_batch = np.array(X_batch), np.array(y_batch)\n",
    "#         for func in preprocess_funcs:\n",
    "#             X_batch, y_batch = func(X_batch, y_batch)\n",
    "        yield np.array(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_generator('MotionEEG_01', batch_size)\n",
    "X_batch, y_batch = next(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\Anaconda3\\envs\\negin\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(63, 80, 10))\n",
    "label = layers.Input(shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\Anaconda3\\envs\\negin\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "x = inp\n",
    "for num_layer in range(1):\n",
    "    filters = min(256, 2**(num_layer+6))\n",
    "    x = layers.Conv2D(\n",
    "        filters=filters, kernel_size=(3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters=filters, kernel_size=(3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters=filters, kernel_size=(3,3), strides=(2, 2),\n",
    "        activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "pre_global_pooled = layers.Conv2D(filters=2, kernel_size=(3,3))(x)\n",
    "global_pooled = layers.GlobalAveragePooling2D()(pre_global_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = global_pooled\n",
    "preds = layers.Softmax()(logits)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "    y_true=tf.one_hot(tf.dtypes.cast(label, tf.int32), 2), y_pred=preds)\n",
    "predicted_label = tf.cast(tf.argmax(preds, -1), tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.dtypes.cast(label, tf.int32), predicted_label), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = tf.trainable_variables()\n",
    "## Change 'trainable_params' for the case of transfer learning\n",
    "gradient = tf.gradients(loss, trainable_params)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "optimizer_op = optimizer.apply_gradients(zip(gradient, trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "intializer = tf.global_variables_initializer()\n",
    "sess.run(intializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.set_learning_phase(1)\n",
    "batch = batch_generator('MotionEEG_01', batch_size)\n",
    "for epoch in range(100):\n",
    "    print('Epoch {}'.format(epoch))   \n",
    "    epoch_training_loss = []\n",
    "    for iteration in range(epoch_size):\n",
    "        if iteration % (epoch_size//10) == 0:\n",
    "            print('\\b.', end='\\r')\n",
    "        X_batch, y_batch = next(batch)\n",
    "        _, batch_loss = sess.run([optimizer_op, loss], {inp: X_batch, label: y_batch})\n",
    "        epoch_training_loss.append(batch_loss)\n",
    "    print('\\nEpoch training loss {}'.format(np.mean(epoch_training_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python negin",
   "language": "python",
   "name": "negin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
